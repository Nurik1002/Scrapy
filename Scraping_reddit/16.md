How to Scrape Data from an E-Commerce Website: The Ultimate Guide for Beginners
Analyze PakWheels Car Listings with Python, Pandas, and Beautiful Soup
Ahmad's Tech Dump
Ahmad's Tech Dump

Follow
6 min read
·
Oct 14, 2024
40


2





Press enter or click to view image in full size

Why Mastering Data Scraping is Essential for E-Commerce Analysis
Data scraping is an invaluable skill in today’s digital landscape. It empowers you to access and analyze website data in ways that go far beyond what’s readily available. This is especially true for e-commerce sites. While many offer basic filters — like price range, date of listing, or product type — these built-in tools can be limiting. By scraping data, however, you unlock a world of deeper insights, allowing for more customized, powerful analysis that can lead to game-changing discoveries.

One challenge with data scraping is that no two websites are designed the same, so there’s no universal approach that works for every e-commerce platform. However, the core process remains similar across sites. In this article, we’ll walk through scraping data from Pakwheels.com, a popular marketplace for second-hand cars. To keep things focused, we’ll narrow our search to listings for a single car model — the Hyundai Sonata — and show you how to extract valuable insights from the data. We will be using Python’s Beautiful Soap library for this purpose.

Following is the link to the Google Colab notebook with the complete code:

Google Colab
Edit description
colab.research.google.com

Press enter or click to view image in full size

Sample page for the Pakwheels
The Code
1. Requisite Libraries
Install and import the following libraries in your code:

import requests
from bs4 import BeautifulSoup
import pandas as pd
import matplotlib.pyplot as plt

# You may also require the following
import numpy as np
import math
from datetime import date
2. Identifying what to scrape
Now generally a scraping tool will scrape the entire (html) page, but you obviously don’t need that. Decide what you require first. For instance, in my case I would require only the part of the page which contains the listings. When you’re done deciding, identify which html div contains this data. For this, go to the developer’s tool throughWindows + Shift + i (on Windows) or right-click and go to inspect element.

Press enter or click to view image in full size

Identifying the part of the screen that I actually require
We notice above that we only require the data inside the html div named ‘col-md-9 grid-style’.

3. Scraping the data
Now that we have defined our base_url and the div we want specifically, we can scrape the data through the following code:

base_url = 'https://www.pakwheels.com/used-cars/search/-/?q=sonata'

response = requests.get(base_url)
soup = BeautifulSoup(response.text, 'html.parser')

# Find all product listings on the current page
productList = soup.find_all("div", class_="col-md-9 grid-style")
The productList variable is a list which contains all data inside the recurring col-md-9 grid-style div.

Become a member
Note: At first, I realized that I was only scraping data from a single page. To collect data from all pages, I used the div element that shows the total number of listings. This allowed me to determine how many pages I needed to scrape. I then created a loop that updated the URL for each page, so it would redirect to the next set of results. For each iteration, I scraped the data from that page and added it to a list. You can find the full code in my Colab notebook.

4. Identifying and Extracting what is required
This part is will consume the maximum time. You’ll have to firstly decide what parts of the data you require and then manually identify where are they placed. For example, I require the Title, Price, KMs driven, Ad posted date, Engine size, Transmission, Model and the Type of the car from the listing. To do this, I would print a single element in the productList list (e.g. productList[0]) and see what are the tags for each. Following is a snippet of what appears when printing the data:

<div class="col-md-9 grid-style">
<div class="">
<div class="search-title-row">
<div class="search-title">
<div class="right">
<div class="price-details generic-dark-grey">

                        PKR 87.95 <span>lacs</span>
</div>
</div>
<a class="car-name ad-detail-path" current-index="0" href="/used-cars/hyundai-sonata-2021-for-sale-in-lahore-9084774" target="_blank" title="Hyundai Sonata  2021 2.5">
<h3 style="white-space: normal;">Hyundai Sonata  2021 2.5 for Sale</h3>
</a>
</div>
</div>
</div>
<div class="row">
<div class="col-md-12 grid-date">
<ul class="list-unstyled search-vehicle-info fs13">
<li>Lahore</li>
</ul>
<ul class="list-unstyled search-vehicle-info-2 fs13">
<li>2021</li>
<li>47,000 km</li>
<li>Petrol</li>
<li>2500 cc</li>
<li>Automatic</li>
</ul>
...
...
...
</div>
After visually identifying the relevant tags, to now extract these details through the code, you need to use the .find(a,b) command. This will take in two arguments, first will be the type of tag (e.g. div, span, li, a) and the second will be the class name of the tag. For example, following is how I extracted the data for some of the fields:

title = product.find("a", class_="car-name ad-detail-path").text.split('\n', 2)[1]
price = product.find("button", class_="phone_number_btn")['data-price']
city = product.find("ul", class_="search-vehicle-info").find("li").text.strip()
vehicle_info = product.find("ul", class_="search-vehicle-info-2")
year = vehicle_info.find_all("li")[0].text.strip()
distance = vehicle_info.find_all("li")[1].text.strip()

## Following elements are in a list so I extracted the specific list from the page first & then used indexing
vehicle_type = vehicle_info.find_all("li")[2].text.strip()  
engine = vehicle_info.find_all("li")[3].text.strip()[:-3]
transmission = vehicle_info.find_all("li")[4].text.strip()
time_posted = product.find("div", class_="pull-left dated").text.strip()
url = product.find("a", class_="car-name ad-detail-path")['href']
Note: I added some additional checks and filters which you can view in the Colab notebook.

Do this for all the elements, i.e. listings, in the productList through a loop, create an object/dictionary out of these and store it in an array, which in my case is data . You can do this through the following:

 data.append({
        "Title": title,
        "Price": price,
        "City": city,
        "Year": int(year),
        "Distance(km)": int(distance),
        "Type": vehicle_type,
        "Engine(cc)": engine,
        "Transmission": transmission,
        "Time_posted(hrs)": convert_to_hours(time_posted),
        "URL(link)": "www.pakwheels.com" + url
    })
5. Viewing the Data
Lastly, convert the data in to a DataFrame and view it.

df = pd.DataFrame(data)
display(df[:10])
Press enter or click to view image in full size

Data for the first 10 listings
6. Analyzing the data
Finally to the most interesting part, data analysis. You can do this using the commands provided by pandas and then print charts using the matplotlib.pyplot library imported in the start. Print charts like the average of the cars, Number of cars using Petrol, average driven KiloMeters etc. Find your the optimal ad through this.

As an easier alternate, you can make use of Colab Notebook’s built in analysis assist. Click on the blue icons on the left of the DataFrame. One will suggest charts for you, and the other helps you in filtering out the data, for example:

Press enter or click to view image in full size

Some of the suggested charts for this data (on Colab)
Press enter or click to view image in full size

Suggested filters for this specific data (on Colab)
Conclusion
Data scraping is a game-changer, especially for e-commerce platforms. While the website’s built-in filters are helpful, scraping gives you full control over the data you want, allowing you to uncover insights that would otherwise go unnoticed. Whether you’re a business owner trying to get an edge or just curious about what you can do with data, scraping is a skill worth mastering. The steps we’ve covered here are just the start — so go ahead, give it a try, and see how it can take your data game to the next level!

Note: Refer to the notebook for detailed, refactored and structured code.

I’m open to suggestions for further enhancements in this code too :)