The real costs of web scraping
After reading this sub for a while, it looks like there's plenty of people who are scraping millions of pages every month with minimal costs - meaning dozens of $ per month (excluding servers, database, etc).

I am still new to this, but I get confused by that figure. If I want to reliably (meaning with relatively high success rate) scrape websites, I probably should residential proxies. These are not cheap - the prices are going from roughly $0.50/1GB of bandwidth to almost $10 in some cases.

There are web scraping API services on the web that handle headless browsers, proxies, CAPTCHAs etc, which costs starts from around ~$150/month for 1M requests (no bandwidth limits). At glance, it looks like the residential proxies are way cheaper than the API solutions, but because of bandwidth, the price starts to quickly add up and it can actually get more expensive than the API solutions.

Back to my first paragraph, to the people who scrape data very cheaply - how do they do it? Are they scraping without proxies (but that would likely mean they would get banned soon)? Or am I missing anything obvious here?

Archived post. New comments cannot be posted and votes cannot be cast.

Upvote
161

Downvote

88
Go to comments


Share
u/free_people_brand avatar
free_people_brand
‚Ä¢
Promoted

For a limited time spend $150, get $100 towards a future purchase. This won't last long!
Shop Now
freepeople.com
Thumbnail image: For a limited time spend $150, get $100 towards a future purchase. This won't last long!
Sort by:

Best

Search Comments
Expand comment search
Comments Section
Haningauror
‚Ä¢
7mo ago
What I do is continue scraping using a proxy, but I block all unnecessary network requests to save bandwidth. For example, when logging in, there's no need to load all the images on the login page, you probably only need the form and the submit button.

Additionally, some scraping tasks are performed via hidden APIs instead of real browser requests, which is highly bandwidth-efficient.



Upvote
69

Downvote

Award

Share

u/OkTry9715 avatar
OkTry9715
‚Ä¢
7mo ago
Some websites (especially sport bookmakers) have ability to detect that you are using API instead of browser and instantly ban you.



Upvote
16

Downvote

Award

Share

Haningauror
‚Ä¢
7mo ago
Yeah, it's basic 101, when developers build an API, they have to protect it. But isn't that like... 80% of the scraping job? Getting around detection? That's what I did with the Shopee API.



Upvote
22

Downvote

Award

Share

Brlala
‚Ä¢
7mo ago
Shopee now throws error in the page when you open the network tab, what‚Äôs the way you got around this to capture network request?



Upvote
2

Downvote

Award

Share

Haningauror
‚Ä¢
7mo ago
Yes, Shopee now detects CDP, I can only say it's possible to get around it with other network capturer tools.


Upvote
6

Downvote

Award

Share

u/theSharkkk avatar
theSharkkk
‚Ä¢
7mo ago
You can use HTTP Toolkit


Upvote
2

Downvote

Award

Share

Lafftar
‚Ä¢
7mo ago
Use burp suite, or Charles proxy or fiddler.


Upvote
1

Downvote

Award

Share

u/LinuxTux01 avatar
LinuxTux01
‚Ä¢
7mo ago
Then found a way around it lol. An http request is still an http request whether done by a browser or a script


Upvote
2

Downvote

Award

Share

4bhii
‚Ä¢
7mo ago
how do you find those hidden apis? like php apis what doesn't even show in network tab



Upvote
3

Downvote

Award

Share

u/vinilios avatar
vinilios
‚Ä¢
7mo ago
if you monitor a browsing session on a website you may find out that most of the information is coming through some kind of api rest calls, if you analyse these calls you can reproduce the communication and extract needed information via these calls with no browser overhead


Upvote
19

Downvote

Award

Share

u/fftommi avatar
fftommi
‚Ä¢
7mo ago
John Watson Rooney on YouTube has some really great vids explain stuff like this

https://youtu.be/DqtlR0y0suo?si=gdpX3xiYrBbCnCZU


Upvote
5

Downvote

Award

Share

Haningauror
‚Ä¢
7mo ago
Well, if it's MVC, there's no way around it. But most websites, especially complex ones, call their APIs for data instead of serving it through PHP.


Upvote
2

Downvote

Award

Share

u/deadcoder0904 avatar
deadcoder0904
‚Ä¢
7mo ago
there's no need to load all the images on the login page, you probably only need the form and the submit button.

how do you know the image isn't captcha? just through manual flow?

i've never heard about this before but damn its pretty dang good insight.



Upvote
1

Downvote

Award

Share

Haningauror
‚Ä¢
7mo ago
If it's a CAPTCHA, it will have a CDN path, class, or ID that indicates it's a CAPTCHA. If I detect it, I just skip the blocking part. Funnily enough, on a poorly designed website, I once blocked the CAPTCHA's JS request and it bypassed it, lol. Not going to work on well-equipped websites, though.


Upvote
5

Downvote

Award

Share

u/albert_in_vine avatar
albert_in_vine
‚Ä¢
7mo ago
I recently made around 2 million requests using ISP proxies that cost me about $3 per week with a 250GB bandwidth cap. The API I was calling only used about 5GB, so bandwidth really depends on the website. Just my two cents, ISP proxies are pretty reliable, but datacenter proxies are the worst; they get detected almost instantly.



Upvote
19

Downvote

Award

Share

u/aaronn2 avatar
aaronn2
OP
‚Ä¢
7mo ago
"Just my two cents, ISP proxies are pretty reliable, but datacenter proxies are the worst; they get detected almost instantly."
I'm not very very experiences in this field, but for that price of $3/week for an ISP - isn't ISP provide 1 or 2 proxies? So effectively, you are still using that 1 or 2 proxies to scrape 2M requests? I thought that this would be a red flag for the administrators of that website and they would ban that IP.



Upvote
5

Downvote

Award

Share

u/albert_in_vine avatar
albert_in_vine
‚Ä¢
7mo ago
You can choose the number of proxies based on the pricing. I used around 20 proxies and since you can refresh them 3 times, that gave me about 60 in total. I also set up a browser fingerprint, and so far, I haven‚Äôt been banned.



Upvote
6

Downvote

Award

Share

seateq64
‚Ä¢
7mo ago
2m requests from 60 proxies sounds quite risky. The website must be having quite low level protection

Usually websites have limit on requests from a single IP per minute. If u reach that number - IP gets blocked



Upvote
2

Downvote

Award

Share

u/uxgb avatar
uxgb
‚Ä¢
7mo ago
If you are crawling many different sites (not just hundreds of thousands of of pages in a single site) you can add some logic to spread out your requests over time when they hit the same site or hosting provider. That way you don't really hit the "x request per minute". Basically do one page for each site first, then 2nd page of each site, etc. It can become more tricky if you need sticky sessions but the basic principle still applies.


Upvote
2

Downvote

Award

Share


[deleted]
‚Ä¢
7mo ago
u/robertmekking avatar
u/robertmekking
‚Ä¢
Promoted

How I went from 3 product listings per day to 500!
How I went from 3 product listings per day to 500!
How I went from 3 product listings per day to 500!
How I went from 3 product listings per day to 500!
How I went from 3 product listings per day to 500!
catalister.com
Sign Up
[deleted]
‚Ä¢
7mo ago
You just cannot scrape at large scale without proxies.



Upvote
11

Downvote

Award

Share

u/ruzigcode avatar
ruzigcode
‚Ä¢
7mo ago
Yes, proxies is a must-have component in web scraping.


Upvote
2

Downvote

Award

Share

hanktertelbaum
‚Ä¢
7mo ago
Can you explain large scale? What/where do the constraints come into play?


Upvote
2

Downvote

Award

Share

u/Pigik83 avatar
Pigik83
‚Ä¢
7mo ago
We scrape at our company 1 billion of product prices per month, more or less. Our proxy bill never went above 1k per month.

The truth is that by rotating IPs by using cloud providers‚Äô VMs, you can scrape 60/70 % of the e-commerces out there.



Upvote
11

Downvote

Award

Share

u/aaronn2 avatar
aaronn2
OP
‚Ä¢
7mo ago
I assume "1 billion of product prices" != 1 billion requests, right?

Shall I ask you what do you mean by "rotating IPs by using cloud providers‚Äô VMs"? Specifically cloud providers' VMs?



Upvote
2

Downvote

Award

Share

u/Pigik83 avatar
Pigik83
‚Ä¢
7mo ago
Correct, but we‚Äôre still talking about several million requests per day. You basically have two ways:

create an automation that deploys your scrapers to a newly created VM and executes it. At the end of the execution, VM is killed

use a proxy manager that spawns the VMs for you and configures them as a proxy, rotating them.


Upvote
6

Downvote

Award

Share

u/RobSm avatar
RobSm
‚Ä¢
7mo ago
How do you rotate VMs at scale?



Upvote
1

Downvote

Award

Share

u/Pigik83 avatar
Pigik83
‚Ä¢
7mo ago
As mentioned in another comment, you simply create and kill VMs where you upload the code and run it. Or you can use a proxy manager that spawns them for you and rotate them.

Consider you can use different could providers at the same time



Upvote
8

Downvote

Award

Share

u/RobSm avatar
RobSm
‚Ä¢
7mo ago
Sure, I am more interested in exact tools you use to manage VM spawning and termination. Feel free to DM if you don't want to mention brands. Thanks.


Upvote
2

Downvote

Award

Share

ish099
‚Ä¢
7mo ago
VMs are very hardware expensive and difficult to scale, why don't you consider using containerization instead


Upvote
1

Downvote

Award

Share


[deleted]
‚Ä¢
7mo ago
u/askolein avatar
askolein
‚Ä¢
7mo ago
Why just mention the proxy? Seems like the sites you scrape are not that defended. How about the rest (VM and DBs)?



Upvote
1

Downvote

Award

Share

u/Pigik83 avatar
Pigik83
‚Ä¢
7mo ago
Of course in the remaining 20% of the websites you have antibots and then you have to choose from site to site if it‚Äôs better to use unblockers or a custom solution.

Our cloud bill ranges between 5-7k per month, split in different providers. This is because all the executions of the scrapers are on the cloud, as the DB



Upvote
1

Downvote

Award

Share

u/askolein avatar
askolein
‚Ä¢
7mo ago
Sounds similar to my company


Upvote
2

Downvote

Award

Share

u/PriceScraper avatar
PriceScraper
‚Ä¢
7mo ago
I own my own bare metal and built my own proxy network. Other than electricity and ISP fees it‚Äôs all a sunk costs paid off many years ago.



Upvote
8

Downvote

Award

Share

u/aaronn2 avatar
aaronn2
OP
‚Ä¢
7mo ago
I am very interested to learn about the proxy network. How and/or where do you source it? How much do you pay for it on a monthly basis? Isn't it that you need to regularly check if the proxies are still working, so you removed the invalid ones from your pool?


Upvote
6

Downvote

Award

Share

u/JitStill avatar
JitStill
‚Ä¢
7mo ago
Same. This seems interesting.


Upvote
1

Downvote

Award

Share

u/informs_org avatar
u/informs_org
‚Ä¢
Promoted

[Discount Code] Join INFORMS and gain your competitive advantage!
u/surfskyofficial avatar
surfskyofficial
‚Ä¢
7mo ago
In our infrastructure, scrape over 10M pages daily. It's not always cost-effective to use residential proxies for server requests and assets. With some outdated or easy-level antibot systems, you can extract cookies and use cheaper server proxies until they expire. You can also use a hybrid approach where xhr / fetch requests are executed using less expensive proxies. Server proxies can be purchased for less than $0.05 per unit each with unmetered 100+ Gbps (over 10x savings).

As mentioned above, it's good practice to block unnecessary resources. If using Chrome / Chromium, you can pass the --proxy-bypass-list flag without the need for filtering in your framework like Playwright / Puppeteer. If you still need to load assets, you can add a shared cache that can be reused between browser instances.

If you frequently work with the same website and use a headless browser, reuse the session and store cache, cookies, local storage, and sometimes service workers.

This above save up to 90-95% of traffic costs. For complex websites, at 1M requests, you can save around $950 on proxies alone, and at $0.5/GB, about $30-40.

The RTT between your scraping infra and the upstream API / proxy servers is also important. Every interaction with the page, including seemingly simple ones, may trigger multiple CDP, which increases the RTT. You can typically achieve at least 2x latency reduction by placing servers in the right geographic locations and data centers, sometimes even achieving 5x improvement.

There are more ways to decrease costs at scale, e.g. using anti-detect browsers, pipelines, warmed-up browsers, but that's another story.


Upvote
8

Downvote

Award

Share

Oblivian69
‚Ä¢
7mo ago
I had to bump up aws resources because of web scraping. 1 day and $250 later I implemented fail2ban. If they would have been polite and not hammer the servers they could still be scrapping my stuff



Upvote
5

Downvote

Award

Share

u/Not_your_guy_buddy42 avatar
Not_your_guy_buddy42
‚Ä¢
7mo ago
i had to scroll SO far down to find the first view from the victim side of scraping , but to anyone paying bandwidth cost scrapers are basically the plague lol and this thread is a bit of a "Are we ze Baddies, Hans" xD


Upvote
2

Downvote

Award

Share

thefirstfedora
‚Ä¢
7mo ago
That's interesting, I had a website ban my ip after 4 failed login attempts (sometimes less) but they failed for unknown reasons because the login credentials were correct. So you could be accidentally banning actual users lol


Upvote
2

Downvote

Award

Share

iamzamek
‚Ä¢
7mo ago
Remindme! 48 hours



Upvote
3

Downvote

Award

Share

u/RemindMeBot avatar
RemindMeBot
‚Ä¢
7mo ago
I will be messaging you in 2 days on 2025-05-13 08:13:53 UTC to remind you of this link

CLICK THIS LINK to send a PM to also be reminded and to reduce spam.

Parent commenter can delete this message to hide from others.

Info	Custom	Your Reminders	Feedback

Upvote
1

Downvote

Award

Share

ConsiderationHot8106
‚Ä¢
7mo ago
Why?



Upvote
0

Downvote

Award

Share

u/Furrynote avatar
Furrynote
‚Ä¢
7mo ago
So he can read the responses after some time and soak up some knowledge


Upvote
7

Downvote

Award

Share


[deleted]
‚Ä¢
7mo ago
shantud
‚Ä¢
7mo ago
I make my own chrome extensions using cursor for every website I want to scrape. Automate Injecting js code to do all work and save json data locally. Instead of proxies. I use android apps (their ips) connected to my wifi to keep changing ips to not get the privilege of getting blacklisted. Ik it is very slow to do this, to manually load pages, manually change proxies after every 70-100 pages, scroll like a human user, then inject code to get json data locally. But I don't like the target website getting loaded with requests after which they'll definitely work on their anti scraping measures. I like to replicate real users, somehow it feels ethical to me.



Upvote
3

Downvote

Award

Share

u/surfskyofficial avatar
surfskyofficial
‚Ä¢
7mo ago
it's important to consider that methods that allow injecting and executing custom js like playwright's addInitScript may be detected by the website in some cases.


Upvote
3

Downvote

Award

Share

u/Axelblase avatar
Axelblase
‚Ä¢
7mo ago
I don‚Äôt understand when you say you use android apps. You mean you use multiple phones to access a webpage through your WiFi network?



Upvote
2

Downvote

Award

Share

Local-Hornet-3057
‚Ä¢
6mo ago
If you got an answer to this part I'd like to know if it's not a problem.


Upvote
1

Downvote

Award

Share

didanet
‚Ä¢
7mo ago
Hey, u/shantud! Great idea. Could you shed some light on how you made it? I'm working on a project that needs to scrap 40-50 websites



Upvote
1

Downvote

Award

Share

shantud
‚Ä¢
7mo ago
Just use any ai to code the chrome extension.
Start with "code me an extension for <site> for this these data."
As you move forward provide 2-3 whole pages source code from the products/pages of the target website to the ai so that it can distinguish between the elements to find the proper selectors to get the data.
Make sure you give the ai prompts like 'separate window for the chrome extension when invoked' also for opening the target website links so that instead of the extension being on the same page it could work as a separate tool even when the page it was invoked on is closed.
Keep taking backups of the source code as you're building.
+Many other things.


Upvote
1

Downvote

Award

Share

u/moiz9900 avatar
moiz9900
‚Ä¢
7mo ago
Remind me 24 hours!


Upvote
2

Downvote

Award

Share

jlg30730
‚Ä¢
7mo ago
Remind me 24 hours


Upvote
2

Downvote

Award

Share

u/foeffa avatar
foeffa
‚Ä¢
7mo ago
Remindme! 24 hours


Upvote
1

Downvote

Award

Share

cgoldberg
‚Ä¢
7mo ago
If you are scraping at scale, you are paying for infrastructure.



Upvote
1

Downvote

Award

Share

u/aaronn2 avatar
aaronn2
OP
‚Ä¢
7mo ago
I understand that it costs money. When reading through this sub-reddit, I somehow got an impression that the professional individuals pay basically close to zero in costs, while when I look at prices of some API solutions or residential proxies, the costs are quite significant, especially when making 10M+ requests per month.



Upvote
1

Downvote

Award

Share

cgoldberg
‚Ä¢
7mo ago
You got the wrong impression. Nobody is doing data collection at scale and paying zero for infrastructure.


Upvote
3

Downvote

Award

Share


[deleted]
‚Ä¢
7mo ago
u/wannabe_kinkg avatar
wannabe_kinkg
‚Ä¢
7mo ago
what are you guys doing with it? I know how to scrap too but not working anywhere, is there anything I could do if I do it myself?


Upvote
1

Downvote

Award

Share

External_Skirt9918
‚Ä¢
7mo ago
If you are from india and i would suggest to use tailscale and connect your broadband router to the VPS. If IP is blocked just turn it off and on the router to get new ip and im scrapping here like a hell with that. They are providing me 3TB of bandwidth per month and paying 7$ for broadband and VPS per month 50$ with spec of 4 core and 12GB obviously its from lowendtalk openvz from TNAHOSTING üòÅ



Upvote
1

Downvote

Award

Share

u/apple1064 avatar
apple1064
‚Ä¢
7mo ago
üòÅ


Upvote
1

Downvote

Award

Share

sdjnd
‚Ä¢
5mo ago
But won't your personal ip get blocked even when using tailscale?



Upvote
1

Downvote

Award

Share

Odd_Insect_9759
‚Ä¢
5mo ago
It will be blocked. I will turn off and on the router. It will give you new ip


Upvote
1

Downvote

Award

Share

u/askolein avatar
askolein
‚Ä¢
7mo ago
In reality scraping at a moderate scale immediately costs 1-5k/month and large scale real time scraping can cost easily 10-50k/month in larger orgs, without data pipeline and engineering considerations. I am conservative here. Senior data engineer.



Upvote
1

Downvote

Award

Share

u/aaronn2 avatar
aaronn2
OP
‚Ä¢
7mo ago
Hello, and thank you. What number of requests do you consider "moderate scale" per month? 1M, or 5M, or 10M? And large scale?

By data pipeline - do you mean by that extracting details from the scraped information and cleaning it up before saving it to the database?



Upvote
1

Downvote

Award

Share

u/askolein avatar
askolein
‚Ä¢
7mo ago
moderate scale is 1M per day I would say.

large scale are in billions generally, per month. depends on how you define datapoints but it's generally like that.

Data pipeline: yes, all the ETL process, the databases, the s3 buckets, the various monitoring systems, the VMs to run it all and any orchestration on top of it (k8s, k3s, if any.)


Upvote
3

Downvote

Award

Share


[deleted]
‚Ä¢
7mo ago

[deleted]
‚Ä¢
7mo ago

[deleted]
‚Ä¢
6mo ago
‚Ä¢
Edited 6mo ago
GoolyK
‚Ä¢
5mo ago
Great question. Your confusion about the low cost figures makes sense because they often leave out the most important part of the strategy.

The secret is that nobody doing serious volume affordably is paying the per gigabyte fees for those big residential proxy networks. The real strategy is to use dedicated datacenter or ISP proxies. For many sites fast proxies from a reputable datacenter are perfectly fine. You can get these for a flat monthly fee with unlimited bandwidth which gives you a predictable low operational cost.

For tougher targets you can build a fallback system. You use the cheap datacenter proxies for almost all requests. If one fails your system automatically retries with a higher trust mobile proxy. It is worth researching how mobile IPs work because anti bot systems are very reluctant to block them. They are highly effective.

The problem then is not bandwidth cost but management. The challenge is rotating thousands of your own proxies and handling complex fallback logic without it becoming a nightmare.

So the formula is cheap dedicated IPs plus a smart management system. That is how you get to millions of pages without spending a fortune.

Hope that helps.


Upvote
1

Downvote

Award

Share

