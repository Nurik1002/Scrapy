After 2 months learning scraping, I'm sharing what I learned!
UPDATE: I built a tool to make this super easy: https://eddie.surf - no more scraping/AI headaches!

Don't try putting scraping tools in Lambda. Just admit defeat!

Selenium is cool and talked about a lot, but Playwright/Puppeteer/hrequests are new and better.

Don't feel like you have to go with Python. The Node.JS scraping community is huge! And more modern advice than Selenium.

AI will likely teach you old tricks because it's trained on a lot of old data. Use Medium/google search with timeframe < 1 year.

Scraping is about new tricks, as Cloudflare, etc block a lot of scraping tactics.

Playwright is super cool! A lot of MS coders brought on from Puppeteer, from what I heard. The stealth plugin doesn't work, however (most stealth plugins don't, in fact!)

Find out YOUR browser headers

Don't worry about fancy proxies, etc if you're scraping lots of sites at scale. Worry if you're scraping lots of data from one site, or regular data scraping from one site.

If you're going to use proxies, use residential ones! (Update: people have suggested using mobile proxies. I would suggest using data center, then residential, then mobile as a waterfall-like fallback to keep costs down.)

Find out what your browser headers are (user agent, etc) and mimic the same settings in Playwright!

Use checker tools like "Am I Headless" to find out some detection.

Don't try putting things in Lambda! If you like happiness and a work/life balance.

Don't learn scraping avoidance techniques from scraping sites. Learn from the sites that teach detecting these!

Put a random delay between requests, 800ms-2s. If the scraping errors, back off a little more and retry a few more seconds later.

Browser pools are great! A small EC2 instance will happily run about 5 at a time.

Archived post. New comments cannot be posted and votes cannot be cast.

Upvote
358

Downvote

102
Go to comments


1

Share
Sort by:

Best

Search Comments
Expand comment search
Comments Section
ivanoski-007
‚Ä¢
1y ago
Learn lxml for python , it's extremely fast

Learn how to find apis in websites and how to get the data you want

Learn threading, to do concurrent requests (on sites that handle it)

Selenium is extremely unstable, and sucks resources, it is better to not rely in it, (I use it to get cookies and then I close it)

Use your headers, or you'll get banned quickly

Learn how to do request get and request post with headers



Upvote
40

Downvote

Award

Share

dedikado
‚Ä¢
1y ago
any tips on finding apis in websites?



Upvote
4

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
Network tab of dev tools!



Upvote
10

Downvote

Award

Share

u/Glittering_Push8905 avatar
Glittering_Push8905
‚Ä¢
1y ago
I want to exactly learn this systematically but I could never find resources



Upvote
3

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
‚Ä¢
Edited 1y ago
selective fall run workable familiar chief innocent provide repeat wild

This post was mass deleted and anonymized with Redact



Upvote
7

Downvote

Award

Share

u/Glittering_Push8905 avatar
Glittering_Push8905
‚Ä¢
1y ago
Wow thank you so much



Upvote
1

Downvote

Award

Share

u/SaoolDaLegend avatar
SaoolDaLegend
‚Ä¢
1y ago
Hey, what was the service he suggested? The original comment got deleted.


Upvote
1

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
What did you suggest here?


Upvote
1

Downvote

Award

Share

u/YourKoolPal avatar
YourKoolPal
‚Ä¢
10mo ago
u/-267- What did you suggest here?
This post was mass deleted and anonymized with Redact


Upvote
1

Downvote

Award

Share

ivanoski-007
‚Ä¢
1y ago
Not all sites have them, some do and some don't



Upvote
1

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
Yes, you are right. I had same problem. i get blocked with captchas and so on. Most websites have mobile Apps and you can install the apps on rooted android phones/emulators and use proxy like http toolkit/Chales to sniff the apis. Plus points:

the server can‚Äôt distinguish between apps and bot and you dont have to deal with captchas .

structured data


Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
Load developer tools in Chrome and find the network tab, look at the things being loaded in there and see if any return structured data like JSON. Then you can copy that URL and reverse engineer by looking at pagination (page=3, start=2024-03-01, etc, etc...)


Upvote
1

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
or learn to root android phone and sniff the api used by the website‚Äùs apps to scrape directly through api without getting blocked.



Upvote
2

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
Great idea and advice! That's next level reverse engineering and very forward-thinking, I love it!



Upvote
2

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
Yeah, i use it already to create bot or track some websites. ;). crawling by using API will not get you blocked and you can bypass captchas .


Upvote
1

Downvote

Award

Share

u/Glittering_Push8905 avatar
Glittering_Push8905
‚Ä¢
1y ago
How to sniff the api



Upvote
1

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
Use Chales proxy and install Charles root ca on rooted android phone. You can see all traffic in plain text in Charles.



Upvote
2

Downvote

Award

Share

u/balanciagas avatar
balanciagas
‚Ä¢
1y ago
can setup & use mitmproxy on your mac/linux to achieve the same result for iphone



Upvote
1

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
I dont know, if it can be done with iphone. Android dont use user certificates by default.



Upvote
1

Downvote

Award

Share

u/BeginningWaltz9766 avatar
BeginningWaltz9766
‚Ä¢
1y ago
Toy can install certs on iPhone too.


Upvote
1

Downvote

Award

Share


2 more replies
ivanoski-007
‚Ä¢
1y ago
What does rooting android have anything to do with it?



Upvote
1

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
because it is not possible to use custom CA with Apps traffic without rooting the phone and use this magisk addon: https://github.com/NVISOsecurity/MagiskTrustUserCerts



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
I feel technically you could set up a laptop as a hotspot/proxy with a log of traffic and capture the requests this way also...



Upvote
1

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
or use android emulator with vanilla Android and http toolkit . works flawlessly :)


Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
Great advice!


Upvote
1

Downvote

Award

Share

u/Additional-Target874 avatar
Additional-Target874
‚Ä¢
1y ago
‚Ä¢
Edited 1y ago
On a site to know drug data and prices, you search for the name or letter, and the search result appears. Does anyone have an idea on how to do web scraping? In order to know how to access the entire drug database or how to create an interface in any language, I write the name of the drug and it searches on this site and the result appears in the interface that I created. Thank you.



Upvote
1

Downvote

Award

Share

ivanoski-007
‚Ä¢
1y ago
You either use the search function, have a list of url, find a hidden api or make a web crawler


Upvote
1

Downvote

Award

Share

lopnax
‚Ä¢
1y ago
Residential proxies are cheaper but some website/app have a database of those ones. If you don‚Äôt want problems and you can pay a bit more go mobile proxies.



Upvote
11

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
Good advice, and noted! I think a combination of both is good. I'd suggest having a fallback method so if a request fails, to use residential and finally mobile to keep costs/usage down.


Upvote
3

Downvote

Award

Share

u/krimpenrik avatar
krimpenrik
‚Ä¢
1y ago
Most important tip, before scraping the content see via chrome inspector where the frontend fetched the data from the backend. Tapping into that endpoint gives you the exact structure you are looking for. If that fails then reverse engineer that structure from the rendered content.

If you find that endpoint you don't need puppeteer for the rendered pages.



Upvote
7

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
That's a good idea! API is of course the best way to interact with the data, but also if you're scraping just the one site and they don't have any mechanisms to prevent you doing this way (like a CSRF/signed token or something) then that's perfect!


Upvote
1

Downvote

Award

Share

u/replicatedhq avatar
u/replicatedhq
‚Ä¢
Promoted

Follow these docs, don't skip a single step.... run these 5 scripts in order.... spin up this cluster... oh no you're stuck?
Learn More
replicated.com
Thumbnail image: Follow these docs, don't skip a single step.... run these 5 scripts in order.... spin up this cluster... oh no you're stuck?
one-escape-left
‚Ä¢
1y ago
What have you been scraping? Would you share any of the code you wrote?



Upvote
5

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
I built a tool that you could ask any questions about a company, or many companies at once. It would then go to the website and find the relevant data, and provide a response. You could also give it multiple companies/sites and ask it to find specific information, or filter companies by criteria. Essentially it was to help us find and qualify leads. It used gpt-4o-mini. I'm going to release as open source once I've tested and refined some more!



Upvote
23

Downvote

Award

Share

lopnax
‚Ä¢
1y ago
‚Ä¢
Edited 1y ago
Is it worldwide? What type of information gives? Structured data?



Upvote
3

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
Worldwide, and you can ask gpt4o mini to produce structured json, it's good at it too! I sometimes use it to parse output from other models that sometimes fail.


Upvote
3

Downvote

Award

Share

u/rclabo avatar
rclabo
‚Ä¢
1y ago
Any way to get in a list to be notified when you release it open source?



Upvote
2

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
I'll probs announce on X when I do, but don't have a list at the moment sorry.


Upvote
2

Downvote

Award

Share

u/deadcoder0904 avatar
deadcoder0904
‚Ä¢
1y ago
Wow, how much did this cost? Also, what were the gtp-4o cost? I assume the latter is cheap af.



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
I use gpt4o-mini which is super cheap. You can also use BeautifulSoap or Cheerio to parse HTML first and remove a lot of noise/attributes from tags/etc that you won't likely need. I also put in code to strip out scripts, css, RSS, other assets, etc as I didn't want to send it to gpt4o-mini. GPT4o mini is good to use because it doesn't have a huge LLM with lots of parameters to sift through (hence it is cheap too) - it's the NLP you're after really, which is great for such a lightweight model - essentially you're only requesting basic reasoning and parsing data you give it, so it's good for this purpose.
I think it cost about $1 to parse around 600 sites, including sub pages. I put logic in to only get to the pages that hold the information I need so it wasn't parsing EVERYTHING!


Upvote
5

Downvote

Award

Share

u/chefkoch-24 avatar
chefkoch-24
‚Ä¢
1y ago
What would you recommend instead of lambda for scheduled jobs?



Upvote
5

Downvote

Award

Share

u/Single_Advice1111 avatar
Single_Advice1111
‚Ä¢
1y ago
‚Ä¢
Edited 1y ago
A raspberry pi. Hook it up with a free account for rabbitmq or lavinmq and send it back to your server.

You‚Äôll need:

A raspberry pi with Docker

A working api in the cloud to receive data from your workers

A place to host your message queue - this can easily be found by using google and there are lots of free options.

A docker container or script in your raspberry pi that consumes jobs and sends the results to your api.

Quite simple tbh



Upvote
12

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
Lambda is just not so good dealing with web drivers, etc and many people have tried hacking it around to get it on lambda. We did get selenium on it after a lot of hassle but even then it's old Chromium and discovered most sites can detect this now, it's just not worth it.
I use an EC2 instance I spin up just for the exercise, ran from a docker container, and shut it down again after I've finished using it. I tried the small EC2 on AWS free tier but it kept crashing it unless 1 site at a time, so I span up a lightweight small server (can't remember exact specs!)
You can use EKS/Fargate/etc to spin these servers up at scale, I believe it will work ok in this if you want the same kind of results as lambda. Costs a bit more but ultimately still scaleable/"serverless". I haven't tried this route yet though. I have a guy on my team that seems to think it's quite easily possible, however!



Upvote
2

Downvote

Award

Share

u/youdig_surf avatar
youdig_surf
‚Ä¢
1y ago
Lambda is crap to setup if you want to add library, i spend too much Time packing a docker container with aws linux distro for this mess and setting it up just to get a random error at this end. ü•≤



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
Yes, Lambda is a very limited "virtualized" environment, so complex things like drivers, etc. don't work so well. Especially when it comes to rendering or things that would typically use the graphics card or other complex hardware features. It is possible, but not worth the effort because by the time you've packaged the driver and got it working, a new version is out and required to do the job effectively.


Upvote
2

Downvote

Award

Share

DataShack
‚Ä¢
1y ago
I've been doing web scraping for almost a decade. I admit that trying to do it in Lambda is the biggest mistake of my life.



Upvote
4

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
I have such a love/hate relationship with it!


Upvote
1

Downvote

Award

Share

RacoonInThePool
‚Ä¢
1y ago
Never use lambda before, why lambda is the biggest mistake to you


Upvote
1

Downvote

Award

Share

u/softwaremansion avatar
u/softwaremansion
‚Ä¢
Promoted

Try Radon PRO for free & enjoy the best IDE for building React Native & Expo apps!
Start your free trial today
Start your free trial today
Start your free trial today
Start your free trial today
Start your free trial today
Start your 14-day trial
ide.swmansion.com
Learn More
u/OP_will_deliver avatar
OP_will_deliver
‚Ä¢
1y ago
Thank you!

Can you share how to do this?

Find out YOUR browser headers

Find out what your browser meta data is (user agent, etc) and mimic the same settings in Playwright!



Upvote
2

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
If you look up "what are my browser request headers" in Google, loads of sites offer this.

Here is the code I use to initiate Playwright:

const browser = await chromium.launch({
args: ["--no-sandbox", "--disable-setuid-sandbox"],
ignoreHTTPSErrors: true,
});



browser = await getBrowser(browserIndex);
if (!browser) {
console.error("Browser instance is null or undefined");
throw new Error("Failed to get a browser instance");
}
context = await browser.newContext({ userAgent }); // userAgent is my agent set earlier
page = await context.newPage();
await page.setViewportSize({ width: 1900, height: 728 });
await page.setExtraHTTPHeaders({
webdriver: "false",
"sec-ch-ua":
'"Chromium";v="128", "Not;A=Brand";v="24", "Google Chrome";v="128"', // Change these based on your headers
"sec-ch-ua-form-factors": '"Desktop"',
"Accept-Language": "en-US,;q=0.9,en;q=0.8", // Change these based on your headers
});
Change the variables of course, based on your headers


Upvote
4

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
What is the use of scrapping can you make money out of it?



Upvote
2

Downvote

Award

Share

ABQFlyer
‚Ä¢
1y ago
Scrapping? Like throwing away? Or scraping?



Upvote
3

Downvote

Award

Share

[deleted]
‚Ä¢
1y ago
Sorry. Web scraping.



Upvote
1

Downvote

Award

Share

OkuboTV
‚Ä¢
1y ago
Plenty of people would pay to have qualified leads. Doctors, Dentists, Lawyers. That's just a specific niche. Businesses would pay to get data on competitors. You can get a good amount of data from public sites through web scraping.

Scraping is just accumulating data. Data is data. There's an inifinite amount of use cases with good data.


Upvote
1

Downvote

Award

Share

u/H4SK1 avatar
H4SK1
‚Ä¢
1y ago
13. Can you give a few example of sites you recommend to learn from?

2. In which way do you think Selenium is worse than other browser scraping library, beside asycn?



Upvote
0

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
13. I used Medium (paid subscription) and also google search a lot, I filtered results only in 2024 as I wanted recent knowledge only. In all honesty, the advice generally isn't very good out there... it's all quite dated. Some say use the stealth plugin, etc which doesn't even work. I would suggest trial and error. It's your best friend in this!

2. "Worse" is perhaps not the right word, but it's quite dated, and I feel Playwright/Puppeteer/hrequests have been built with a more modern approach. It's my opinion, not necessarily fact. The main reason is that a lot of traditional scraping techniques were based on Selenium, so finding modern/accurate/reliable tutorials/guidance is VERY hard. You could be digging out an article generated by AI that was based on data from 10-15 years ago that is now mostly-redundant. I followed so many tutorials that took me to dead-ends!


Upvote
5

Downvote

Award

Share

khanosama783
‚Ä¢
1y ago
please share some articles if possibly


Upvote
1

Downvote

Award

Share

Nora-AR
‚Ä¢
1y ago
¬°Thank you for this! I have some experience with web scraping but I use the old tools cause I feel more comfortable. I am trying to change this and wanna learn more about the new tools üòä


Upvote
1

Downvote

Award

Share

u/Prior_Meal_6228 avatar
Prior_Meal_6228
‚Ä¢
1y ago
Can you explain 13 and 14 a little bit



Upvote
1

Downvote

Award

Share

sudodoyou
‚Ä¢
1y ago
I can provide insight:
13. OP is saying that if you learn how to detect scraping then you‚Äôll be better at avoiding detection. If you merely try to learn to learn avoid detection, you‚Äôll likely miss other techniques.
14. If you‚Äôre mimicking normal requests, you will not get a user request exactly every 5 seconds, it will appear to be more randomly distributed. So when you request to pull data from a website, grab the data at random intervals.



Upvote
4

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
Yes, thanks u/sudodoyou - spot on! A delay will mimic the typical user that will load a site, look for a link/content and then click on it. I always try to imagine how a real person would behave on these sites.


Upvote
1

Downvote

Award

Share

Best_Fish_2941
‚Ä¢
1y ago
3. Do you mean nod js better than python or equal



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
It's whatever you are most comfortable with, tbh... I know both and say NodeJS has a more mature community and generally more modern approaches. Because Python is easy and accessible, plus Selenium with python has been around forever, a lot of the online guidance is quite dated. Playwright/Puppeteer is more "recent" so I preferred to go with it.


Upvote
1

Downvote

Award

Share

u/ReceptionRadiant6425 avatar
ReceptionRadiant6425
‚Ä¢
1y ago
Why not lambda function any specific reason?



Upvote
1

Downvote

Award

Share

Sensi1093
‚Ä¢
1y ago
Scraping is not a good fit for lambda because you‚Äôll probably have something to scrape all the time and the load is usually pretty constant.

Lambda shines when you either have spiky load or need/want to scale to zero. Neither of those things apply for the usual Webscraping load.



Upvote
1

Downvote

Award

Share

u/ReceptionRadiant6425 avatar
ReceptionRadiant6425
‚Ä¢
1y ago
For instance my scraper runs only 4 times a day for an average of 10 minutes, still lambda is a bad option for that or not. I do not need to scrape the data 24x7 for the business logic I am currently working on.



Upvote
1

Downvote

Award

Share

Sensi1093
‚Ä¢
1y ago
Sure lambda is fine for that.

Just be aware of the 15m execution limitation and 10GB memory limitation, both are hard limits for lambda. If you plan to go beyond that, maybe look into other solutions which allow short lived / task based execution like ECS



Upvote
1

Downvote

Award

Share

u/ReceptionRadiant6425 avatar
ReceptionRadiant6425
‚Ä¢
1y ago
Sure will do that. Thanks


Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
It doesn't behave too well with the web drivers that Chromium needs to run. Generally complex operations should stay away from Lambda and in something like Fargate.


Upvote
1

Downvote

Award

Share

u/Cultural-Arugula-894 avatar
Cultural-Arugula-894
‚Ä¢
1y ago
Hello, Is there any way to run cheerio or playwright on Heroku server. I am facing issue with these scraping packages on heroku server. Doesn't work.


Upvote
1

Downvote

Award

Share

Salt-Page1396
‚Ä¢
1y ago
"If you're going to use proxies, use residential ones!"

Try datacenter proxies first. They are hell of a lot cheaper than residential and a lot of the times get the job done.



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
Yes, agreed. Suggest data center, then residential, then mobile fallback on failures



Upvote
1

Downvote

Award

Share

u/Glittering_Push8905 avatar
Glittering_Push8905
‚Ä¢
1y ago
But afaik proxy works on monthly subscription not bandwidth used ?



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
Yes, but you pay for the IPs/slots, so say you were scraping 15 sites at once and you had 5 IP addresses to rotate for mobile proxy, you would want to keep them as free as possible for those that need them, or you'd end up with quite a big queue or paying for a lot of IP addresses. Of course; that's only if you're scraping multiple sites. Some charge on bandwidth, some charge on monthly.


Upvote
2

Downvote

Award

Share

u/ConfusionHumble3061 avatar
ConfusionHumble3061
‚Ä¢
1y ago
How fast are the Puppeteer and the other compare to Selenium ?

I'm trying to scrape a website but i cannot do with beautifulsoup and i found myself stuck because a link that i'm scrapping is using some X-Amz-Algorithm key who change everytime



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
I don't suggest to have speed as an objective or you'll trigger defence mechanisms on sites you try to scrape too quickly. My preferred route is to have concurrent/parallel requests to multiple sites at one time.


Upvote
1

Downvote

Award

Share

Salt_Ant107s
‚Ä¢
1y ago
Im so gonna put this in my zotero


Upvote
1

Downvote

Award

Share

Purple-Control8336
Cake icon
‚Ä¢
1y ago
Why Scrap when GTP is already has all data and Google Bard



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
Bard is now Gemini, and together with GPT they have cut-off dates, plus they don't always have all the information or relevant information that you require. If I'm taking the data from a company website, it's probably going to be accurate, whereas if it is scraped from some random person on YouTube (which trained a lot of GPT data), it may not be so accurate. Plus look up AI Hallucination, it's quite common (at least, for now). We published a whole whitepaper on it!



Upvote
1

Downvote

Award

Share

Purple-Control8336
Cake icon
‚Ä¢
1y ago
Thanks makes sense for now. Google is Father of all Data‚Ä¶



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
They've been at it a long time, and store up to 30 versions of a site's page - it's wild, how much data they must have! Then on top the data they collect through their DNS servers, Google Phone, Chrome, Maps, etc - I can't imagine any company that has more data than they do!


Upvote
1

Downvote

Award

Share


[deleted]
‚Ä¢
1y ago
RacoonInThePool
‚Ä¢
1y ago
Can you share more about 12 and 15, first time heard about browser pool



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
12 I have discussed a fair amount in other comments, but generally speaking Lambda is a limited virtual environment with strict hardware restrictions that Chromium generally needs to run.

15:

async function initializeBrowserPool(setConcurrent) {
  MAX_CONCURRENT = setConcurrent;  
  for (let i = 0; i < MAX_CONCURRENT; i++) {
    const browser = await chromium.launch({
      args: ["--no-sandbox", "--disable-setuid-sandbox"],
      ignoreHTTPSErrors: true,
    });
    browserPool.push(browser);
  }
}

async function getBrowser(index) {
  if (browserPool.length === 0) {
    await initializeBrowserPool();
  }
  return browserPool[index % MAX_CONCURRENT];
}

const queue = [];

  for (let i = 0; i < rows.length; i++) {
    const row = rows[i];

    // If we've reached the maximum number of concurrent tasks, wait for one to finish
    if (queue.length >= MAX_CONCURRENT) {
      await Promise.race(queue);
      // Remove the completed task from the queue
      const index = queue.findIndex((p) => p.status !== "pending");
      if (index !== -1) {
        queue.splice(index, 1);
      }
    }

    // Start a new task and add it to the queue
    const browserIndex = queue.length; // Use the current queue length as the browser index
    const task = processRow(sheet, row, i, browserIndex);
    
    queue.push(task);

    // Immediately add error handling so we don't lose track of the promise
    task.catch((error) =>
      console.error(`Unhandled error in task for row ${i}:`, error),
    );

    // Sleep, and on to the next!
    await new Promise(resolve => setTimeout(resolve, 300));

Upvote
2

Downvote

Award

Share

u/pcuser522 avatar
pcuser522
‚Ä¢
1y ago
Where the hell was this post months ago. Beautiful info. Been stuck on all of this issues personally litterally last night solved my proxy issue. W post. My one suggestion is this. If cloudflare is blocking you try to reinitialize the driver for each request and typically I‚Äôve been able to load the site and not get hit for a few seconds so I can scrape the data before getting cut off. Works on a fuck ton of sites.


Upvote
1

Downvote

Award

Share

u/boreagami avatar
boreagami
‚Ä¢
1y ago
Lambda web scraping is working perfect for me with selenium at a large scale.


Upvote
1

Downvote

Award

Share

Limp_Charity4080
‚Ä¢
1y ago
what tools did you use to manage browser pools?



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
I just did this with Node.js using promises but it's also possible in Python. You can concurrent request anything.


Upvote
1

Downvote

Award

Share

faz_Lay
‚Ä¢
1y ago
Node.JS scraping community is huge -- seriously !!!


Upvote
1

Downvote

Award

Share

u/Shad0w_spawn avatar
Shad0w_spawn
‚Ä¢
1y ago
I‚Äôve been learning with playwright and trying to make some ‚Äògeneric‚Äô scrapers. Can you explain 7, 10, and 15 a little more? Do you need to regularly update the headers? Are the browser pools replicable in non AWS envs?



Upvote
1

Downvote

Award

Share

Sea_Cardiologist_212
OP
‚Ä¢
1y ago
I responded to this in another comment with some example code, take a look and let me know if you get stuck


Upvote
1

Downvote

Award

Share


[deleted]
‚Ä¢
1y ago
u/Single_Tomato_6233 avatar
Single_Tomato_6233
‚Ä¢
1y ago
I've found a bit of a workaround to using Playwright in Lambda. The trick is to deploy a server with chrome/playwright on EC2 or a similar platform. Then you can connect to it over CDP from your Lambda:

browser = await pw.chromium.connect_over_cdp(CDP_URL)
Github repo with the Dockerized playwright server here: https://github.com/finic-ai/finic


Upvote
1

Downvote

Award

Share

u/dredav avatar
dredav
‚Ä¢
1y ago
One question about proxies: How often do you rotate them? Are u using a pool of proxies and pick one per request or are you using on proxy per browser pool instance?


Upvote
1

Downvote

Award

Share

Holiday-Regret-1896
‚Ä¢
1y ago
Need help please:

Please check this - https://genius.com/Kendrick-lamar-not-like-us-lyrics

the problem is i cant scrape ANNOTATION  for each sentence there mention

Expecting format like this:
****Ayy, Mustard on the beat, ho

(Genius Annotation

Los Angeles-based producer Mustard‚Äôs signature producer tag is an excerpt of frequent collaborator and Compton artist YG that originated from YG‚Äôs 2011 track ‚ÄúI‚Äôm Good.‚ÄùThis is notable because Drake aligned himself with YG in an attempt to discredit Kendrick‚Äôs street cred in his then-previous diss track, ‚ÄúFamily Matters‚Äù:You know who really bang a set? My nigga YGMustard tweeted the following shortly after the song dropped:I‚Äôll never turn my back on my city ‚Ä¶. and I‚Äôm fully loadedWhile Mustard shot down the rumor of him sampling Nas' ‚ÄúEther‚Äù for the track, the production does feature a sped-up sample from the 1968 track ‚ÄúI Believe To My Soul‚Äù by Monk Higgins:)

Deebo any rap nigga, he a free throw
( Genius Annotation

Deebo, portrayed by Compton actor Tommy Lister Jr., is a fictional character from the iconic 1995 film Friday. He is depicted as a sociopathic bully that no one in the community is willing to stand up to. This parallels Kendrick‚Äôs depiction of Drake in this song and all the previous installments of his Drake diss tracks. However, here, Kendrick is knowingly taking on the persona of a bully. He may also be making a callback to his verse on ‚ÄúLike That,‚Äù where he said, ‚ÄúI‚Äôm snatchin' chains,‚Äù as, in Friday, Deebo snatches a character‚Äôs chain.Deebo is also the nickname of NBA player DeMar DeRozan, who played for The Chicago Bulls when this track dropped. The significance of this is Kendrick‚Äôs parents came from Chicago. Although DeRozan is from Compton, he has a connection to Drake‚Äôs hometown, as he previously played for Toronto Raptors, for whom Drake is an ambassador. Kendrick mentions DeRozan later in the song:I‚Äôm glad DeRoz' came home, y'all didn‚Äôt deserve him neitherDeRozan is a proficient free throw shooter, with his 84.1% career average only 6.9% short of the record. Therefore, Kendrick is implying that beefing with other rappers is as effortless for him as free throws are for DeRozan.DeRozan went on to cameo in the ‚ÄúNot Like Us‚Äù visuals.)

Thank you :)

ps. i am non-coder so reply jargon-free


Upvote
1

Downvote

Award

Share

u/Any_Pirate_7025 avatar
Any_Pirate_7025
‚Ä¢
10mo ago
u/Sea_Cardiologist_212 Intent to download EC2 instances in AWS and working well because, when a failed request, simply eliminate the instance and assign me a new IP. Probe in Hetzner, but you still have to assign the IP misma to create this other instance, and in DigitalOcean you have the misma limitation with the other band. Contracted proxy services, but the majority gives me errors. Did you have a similar experience or recommend a better strategy?