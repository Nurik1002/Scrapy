Anyone here scraping at a large scale (millions)? A few questions.
What’s your stack / setup?

What data are you scraping (if you don’t mind answering, or even CAN answer)

What problems have you ran into?


Upvote
96

Downvote

65
Go to comments


Share
u/mercerfer21 avatar
mercerfer21
•
Promoted

Removing Technical Debt - Salesforce Orgs can become unmanageably complex over time. This problem tends to be the most severe in the oldest and largest Salesforce Orgs.
Learn More
metazoa.com
Thumbnail image: Removing Technical Debt - Salesforce Orgs can become unmanageably complex over time. This problem tends to be the most severe in the oldest and largest Salesforce Orgs.
Join the conversation
Sort by:

Best

Search Comments
Expand comment search
Comments Section
u/RandomPantsAppear avatar
RandomPantsAppear
•
3mo ago
I have a scraped into the billions.

Right now my preferred setup is Django/celery. I upload json to S3, and have have an event occur that makes AWS process the data on upload. For scrapers themselves I run tiny fargate instances on AWS that create the json objects in s3 and scrape. There is a scheduler task that runs every 5 minutes to check the queue size in celery, and based on that scales up or down the amount of fargate cluster instances I have, with a 10 minute delay so they can wrap up their tasks.

I like this because I can basically replay anything I want just by specifying the s3 object, and no server stays active longer than is required.



Upvote
21

Downvote

Reply

Award

Share

u/prometheusIsMe avatar
prometheusIsMe
•
2mo ago
I'm curious, why not an async framework like fastapi instead of django?



Upvote
1

Downvote

Reply

Award

Share

u/RandomPantsAppear avatar
RandomPantsAppear
•
2mo ago
A few reasons.

This was client work and I will normally go for my most familiar frameworks for client work. It makes my timelines more accurate, the client happier and me more confident in my product.

This API wasn’t only serving the scrapers. It was a bit of a monolith that was also handling normal web requests. Django is a reliable tank.

I am pretty competent at handling my own threading tasks and multiple processes. I find that often formal async is more laborious than its worth.



Upvote
1

Downvote

Reply

Award

Share

u/Low_Imagination_4089 avatar
Low_Imagination_4089
•
2mo ago
what is your job?



Upvote
1

Downvote

Reply

Award

Share

u/RandomPantsAppear avatar
RandomPantsAppear
•
2mo ago
I actually just got laid off because the startup ran out of cash. But I’ve used this setup with client work as well.



Upvote
1

Downvote

Reply

Award

Share

u/Sudden-Bid-7249 avatar
Sudden-Bid-7249
•
2mo ago
can you share it in github? I am trying to make a powerful OSINT and this would be a really big help.


Upvote
2

Downvote

Reply

Award

Share


[deleted]
•
2mo ago
•
Edited 2mo ago
u/webscraping-net avatar
webscraping-net
•
3mo ago
We’ve been scraping at this scale for a while now.

Stack: Python + Scrapy, Redis, PostgreSQL, Playwright, running on bare metal with cloud providers.

Some of the bigger sources we’ve handled: Google Maps, Amazon, Lowe’s, Home Depot, TicketMaster, G2, Yelp, Zillow, Redfin, Leboncoin (can’t mention some others due to NDAs).

Biggest challenge recently: our bots started getting banned. Turns out the open-source anti-detect browser we rely on (Camoufox) isn’t being updated because the main dev is in the hospital. Talk about a bus factor.



Upvote
30

Downvote

Reply

Award

Share

SynergizeAI
•
3mo ago
Boltasaurus a worthy alternative?



Upvote
5

Downvote

Reply

Award

Share

u/webscraping-net avatar
webscraping-net
•
3mo ago
Hmm, let me take a look.



Upvote
1

Downvote

Reply

Award

Share

Relative_Rope4234
•
3mo ago
Is zendriver better than camoufox



Upvote
1

Downvote

Reply

Award

Share

u/webscraping-net avatar
webscraping-net
•
2mo ago
Camoufox is bad now because it hasn't been updated for a while. I haven't tried zendriver before.


Upvote
1

Downvote

Reply

Award

Share

u/aaronn2 avatar
aaronn2
•
2mo ago
Love learning this.

How big is the team managing this infrastructure?

What are the infrastructure costs running this (without the human bodies)?

Are you using some scraping API services, or are you doing everything in-house (managing IP proxies, cookies, headers, etc.)



Upvote
2

Downvote

Reply

Award

Share

u/webscraping-net avatar
webscraping-net
•
2mo ago
Managing our infrastructure takes about 5-10 hours per month, and 2 team members can handle maintenance.

Servers cost around $250/month, and proxies about $600/month.

We use paid scraping APIs when our estimated labour costs for bypassing anti bot measures/spinning up a new project are much higher than API's monthly fees.


Upvote
4

Downvote

Reply

Award

Share

u/k1465 avatar
k1465
•
2mo ago
Can you say what you do with the HD and Lowe’s data?



Upvote
1

Downvote

Reply

Award

Share

u/webscraping-net avatar
webscraping-net
•
2mo ago
We use it for ecommerce analytics (stock levels, revenue estimation, pricing trends, that sort of thing).



Upvote
4

Downvote

Reply

Award

Share

u/IamFromNigeria avatar
IamFromNigeria
•
2mo ago
Weldone Bro

I admire your scrapping effort..hope you find a solution for the anti-bot detection


Upvote
1

Downvote

Reply

Award

Share

Global_Gas_6441
•
2mo ago
thanks for sharing


Upvote
1

Downvote

Reply

Award

Share

u/HelloWorldMisericord avatar
HelloWorldMisericord
•
3mo ago
•
Edited 3mo ago
AWS lambda microservices scraping Airbnb pricing data. ~15K listings, ~100 pricing scrapes per listing It currently takes about 2-3 days to fully scrape all listings including retries due to IP blocks. I have my own secret sauce for getting past Airbnb’s IP blocks without having to resort to residential VPNs

Airbnb is an absolute pain to scrape

EDIT: For all those chat requests asking me what my secret sauce is, what I'll say is that I:

Rely on Airbnb API endpoints & creative (and probably against TOS) usage of a number of AWS services which are low to no cost.

Conducted and refined my process through MANY iterations of trial-and-error

Even with all of this, I built in VERY robust error checking and retry logic

Even if I shared my secret sauce, it wouldn't work for anything other than Airbnb and even then for anywhere from 3-6 months. Airbnb regularly breaks my algorithm with changes which requires me to reconfigure things. Most changes are pretty minor needing only the addition of one more quality check, but at one point, I was down for nearly 2 weeks due to major changes in their API. To be fair, it turned out to be a perfect storm of busy personal life and my technical debt finally catching up to me, but the fact remains that I need to keep on top of it.



Upvote
17

Downvote

Reply

Award

Share


[deleted]
•
3mo ago
xmrstickers
•
3mo ago
Interesting. Do you monetize that data for someone else or something?



Upvote
1

Downvote

Reply

Award

Share

u/HelloWorldMisericord avatar
HelloWorldMisericord
•
3mo ago
I offer revenue management services and consulting for Airbnb. To be honest, the amount of data I scrape is overkill (you can get an 80% solution with just 20-30 comparables), but my day job and passion is in data so getting population level insights into the NYC Airbnb market is just cool.



Upvote
3

Downvote

Reply

Award

Share

xmrstickers
•
3mo ago
Very cool. Didn’t even realize that would be lucrative at all. So there’s enough people with poorly priced or optimized rentals that you’re able to approach and offer consultation to maximize revenue for them, for a fee?

Sounds like a really cool niche, nice.



Upvote
2

Downvote

Reply

Award

Share

u/HelloWorldMisericord avatar
HelloWorldMisericord
•
3mo ago
It isn't "lucrative" yet and definitely a net loss when I factor in my time spent. I could significantly simplify (ex. just scraping 20-30 comparables instead of entire population of NYC Airbnb listings), but having population level data (not just samples) is a differentiator I'm relying on vs. competitors like PriceLabs.



Upvote
1

Downvote

Reply

Award

Share

xmrstickers
•
3mo ago
Oh I see. Yeah, that seems to be a huge edge based on competing data sets if you are able to have the entire picture for pure analysis instead of extrapolation


Upvote
1

Downvote

Reply

Award

Share

u/followai avatar
followai
•
2mo ago
I haven’t been to NYC in 2 years. The week I was there the Airbnb law took effect and the only listings in Manhattan were hotels. I take it the situation has changed?


Upvote
1

Downvote

Reply

Award

Share

Wise_Concentrate_182
•
3mo ago
What do your lambda servers have? Python?


Upvote
1

Downvote

Reply

Award

Share

u/vast_ai avatar
u/vast_ai
•
Promoted

Spin up high-end GPUs for ML training. From $0.25/hr. Deploy in seconds.
Spin up high-end GPUs for ML training. From $0.25/hr. Deploy in seconds.
Spin up high-end GPUs for ML training. From $0.25/hr. Deploy in seconds.
Spin up high-end GPUs for ML training. From $0.25/hr. Deploy in seconds.
Spin up high-end GPUs for ML training. From $0.25/hr. Deploy in seconds.
Spin up high-end GPUs for ML training. From $0.25/hr. Deploy in seconds.

cloud.vast.ai
Sign Up
u/Mysterious-Web-8788 avatar
Mysterious-Web-8788
•
3mo ago
It's all about microservices for me. I host one centralized service that's a very lightweight registry of "requests" (things that need to be scraped) and then I spin up N microservices to hit up that centralized service and do the dirty work.

It's elegant really, the microservices don't really need to be efficient as you can just spin up more. The centralized server needs to be efficient, but it's lightweight, pulling properly indexed data out of a database very quickly.

For the microservices, honestly, I just buy up old used optiplex dell workstations off ebay. They are dirt cheap and you can get a good balance of cost and computing power. I'm not sure if it's the most cost effective but going to the cloud for them isn't, cloud cpu cycles are expensive and scraping is going to be cpu intense. I've heard of people using budget SoC's to do it, that might be a route too, but they are more expensive than they used to be and old workstations aren't.

IP issues are always a conflict. Various ways to approach that.



Upvote
17

Downvote

Reply

Award

Share

Jeannetton
•
3mo ago
are you talking about residential IPs etc. ?



Upvote
2

Downvote

Reply

Award

Share

u/Mysterious-Web-8788 avatar
Mysterious-Web-8788
•
3mo ago
those are IP's yeah



Upvote
2

Downvote

Reply

Award

Share

u/Frosty-Ad-6882 avatar
Frosty-Ad-6882
•
3mo ago
i am new to IP blocking, would love to hear something about this from you!!



Upvote
3

Downvote

Reply

Award

Share

u/HelloWorldMisericord avatar
HelloWorldMisericord
•
2mo ago
Not OP, but IMO IP blocking is the most expensive blocking to overcome because it is so simple. The IP you're using is either blocked or not, and for the companies that block IPs based on activity level, scrapers stick out like a sore thumb. The only thing you can do is use a different IP, which usually means residential IPs for the most difficult targets.

With fingerprinting, you can get fancy with your request headers, TLS, JS, etc. to look like a normal web user, and it's a never ending arms race on this side of the playing field. There are definitely much smarter folks who understand this side and are the ones writing the actual code for anti-scraping or scraping tools/libraries. I just use Python libraries like curl_cffi


Upvote
1

Downvote

Reply

Award

Share

u/Playful-Battle992 avatar
Playful-Battle992
•
3mo ago
5k+ scrapes/second split across many thousand domains. Primarily product pages.

A large bunch of nodejs scrapers (slowly migrating to Go) , as well as chromium scrapers, running in a k8s cluster.

Main challenges is keeping proxy costs down, while also maintaining a decent success rate.



Upvote
6

Downvote

Reply

Award

Share


[deleted]
•
2mo ago
u/Smatei_sm avatar
Smatei_sm
•
3mo ago
Java. Apache http client/ okhttp client or selenium+chrome, or a mixed solution. Lately I switched to playwright to replace selenium+chrome.

Scraping bing, yahoo, google maps, YouTube, duckduckgo, amazon shopping, and many other search engines for a web ranking software. +20k proxy pool. We used to scrape google too back in the days this was trivial, but now we switched to serp scraping api providers for google.

We keep everything on Amazon AWS, smaller ec2 machines for plain http client requests and larger ones for browser or mixed solution. Mixed solution is getting the cookie with the browser and then http client is using that cookie. Cookie stored and used for a couple of days or until search engine ban.

After scraping the html is uploaded on S3, a parser service parses the html either with regular expressions or with xpath, or both. The resulting Json is uploaded on S3. Then another service takes the Json from S3, retrieves specific site information, position, page, url, and saves them in AWS Aurora MySQL RDS severs.

When we have problems with scraping, captcha, cloudflare, ... we switch to serp API providers or scraping apis. Some scraping apis provide both simple or browser (more expensive) requests. On the parsing side, we detect when the search engine page changes and either fix regex/xpath manually or we generate them automatically, or both.



Upvote
3

Downvote

Reply

Award

Share

u/aaronn2 avatar
aaronn2
•
2mo ago
This sounds super interesting. Might you outline how much such infrastructure costs per month?



Upvote
1

Downvote

Reply

Award

Share

u/Smatei_sm avatar
Smatei_sm
•
2mo ago
Amazon AWS costs about $50k per month. There are some additional costs with ips and scraping api providers. Most of the traffic is for google, on scraping providers, that is most of the additional cost. A couple of years ago, when we used to scrape google ourselves, the ips were most of the additional costs. About $20k per month until this month. It will be more, as Google requires more requests starting from this month:

https://searchengineland.com/google-search-confirms-it-does-not-support-the-results-per-page-parameter-462244


Upvote
2

Downvote

Reply

Award

Share

u/Silver_5557 avatar
u/Silver_5557
•
Promoted

looking for reliable proxies? We offer Residential, ISP, Datacenter, and Mobile proxies from just $0.75. Great support, and affordable prices!
looking for reliable proxies? We offer Residential, ISP, Datacenter, and Mobile proxies from just $0.75. Great support, and affordable prices!
looking for reliable proxies? We offer Residential, ISP, Datacenter, and Mobile proxies from just $0.75. Great support, and affordable prices!
looking for reliable proxies? We offer Residential, ISP, Datacenter, and Mobile proxies from just $0.75. Great support, and affordable prices!
proxy-seller.com
View More

[deleted]
•
2mo ago
•
Brand Affiliate
u/qzkl avatar
qzkl
•
3mo ago
Python, asyncio, aiohttp, one core service that orchestrates everything like what and when needs to be scraped, reqds/updates db etc. Each website is a separate serivice, like Amazon, Airbnb, Bestbuy, Walmart etc. Theres more than 20 of them. Problem is mainly just maintenance, responses change, endpoints get deprecated, anti bot detection, captchas etc. the usual


Upvote
2

Downvote

Reply

Award

Share

donde_waldo
•
3mo ago
•
Edited 3mo ago
C#. I do everything from a single application, including all scraping and database storing. This is all built into an API I use for myself, so it's continuously serving the data as well. Anytime it's running, it's easily doing a million requests per day. Memory never exceeds 100MB either, CPU usage is very low too. Database is MySQL running on the same machine, which is some HP G3 or something, little square computer under my desk.

Data integrity is the biggest thing for me.

It's a lot to lose over some small mistakes. If you get into updating existing rows where the data itself comes from multiple sources, you end up with half new data, half null data because a request failed, and then your existing old data, and you can't just replace the old data because you only have half the new data.. ugh what a mess. Build a big ole data pipe that swallows anything and never look at it again.


Upvote
2

Downvote

Reply

Award

Share

u/maxim-kulgin avatar
maxim-kulgin
•
2mo ago
scraping 2000+ sites per day.

.net core

e-com shops.

captcha, Cloudflare (paid plan) especially on big e-com sites.

we are using mostly undetected chromium.


Upvote
2

Downvote

Reply

Award

Share

anfy2002us
•
2mo ago
I use Python, Scrapy, Nats, S3, and scrape local news websites, the good thing is that most of them do not invest a lot on tech and of you’re not hitting them hard, they will not block you and I scrape from residential IPs


Upvote
2

Downvote

Reply

Award

Share

u/raydou avatar
raydou
•
3mo ago
RemindMe! One Week



Upvote
1

Downvote

Reply

Award

Share

u/RemindMeBot avatar
RemindMeBot
•
3mo ago
•
Edited 2mo ago
I will be messaging you in 7 days on 2025-10-03 14:57:32 UTC to remind you of this link

1 OTHERS CLICKED THIS LINK to send a PM to also be reminded and to reduce spam.

Parent commenter can delete this message to hide from others.

Info	Custom	Your Reminders	Feedback

Upvote
0

Downvote

Reply

Award

Share

RageQuitNub
•
3mo ago
following


Upvote
0

Downvote

Reply

Award

Share

study_english_br
•
3mo ago
Following


Upvote
0

Downvote

Reply

Award

Share

new-to-gambling
•
3mo ago
Following


Upvote
1

Downvote

Reply

Award

Share

u/IncidentOne6978 avatar
IncidentOne6978
•
3mo ago
RemindMe! One Week


Upvote
1

Downvote

Reply

Award

Share

u/ElPanda avatar
ElPanda
•
2mo ago
I try to keep the tech stack pretty simple. Proof of concept with Python on my local machine before thinking about how to scale based on data-freshness needs. Most of the infrastructure winds up on AWS using various compute resources (Lambda, EC2, Fargate), again based on needs.

Most common problems are keeping up with changing DOM, changes to APIs, overcoming bot detection, rate limiting and IP blocks. Every site is a little different, but you build up a list of go to methods with experience. It's a never ending game of cat and mouse.


Upvote
1

Downvote

Reply

Award

Share

u/Old_Celebration_857 avatar
Old_Celebration_857
•
2mo ago
C# and mixing selenium, html document and regular GP requests.

Just be nice to your host.


Upvote
1

Downvote

Reply

Award

Share

rahulsingh_ca
•
2mo ago
Containerized (docker) scraper and parser scripts that pull from SQS queues. Scraper is built with python. Parser is just standard libraries for json parsing. The payload I get returns a lot of junk so the parser cleans the ~20 fields we want and sends the data to postgres (there's a reason I did this, I have been crucified before by data engineering reddit because I didn't opt for no-sql storage lol). The whole setup is deployed on AWS EKS and each script can scale independently. It get's kicked off by a cron job that loads the queue for the scraper and then the scraper sends the raw payload to the parser queue to get cleaned.

Google Maps

There's not really problems day to day other than housekeeping (updating packages). At the start finding good proxies took time and I had to add compression to the queue messages because some of the payloads were huge.


Upvote
1

Downvote

Reply

Award

Share

u/thereisnospooongeek avatar
thereisnospooongeek
•
2mo ago
following


Upvote
1

Downvote

Reply

Award

Share

